{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.autograd import Variable \n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Model(torch.nn.Module): \n",
    "  \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(2, 1)  # One in and one out \n",
    "        \n",
    "    def forward(self, x): \n",
    "        y_pred = self.linear(x) \n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "columns = boston.feature_names\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[65.2000, 78.9000],\n",
      "        [61.1000, 45.8000],\n",
      "        [54.2000, 58.7000],\n",
      "        ...,\n",
      "        [ 9.6900, 11.9300],\n",
      "        [11.9300, 11.9300],\n",
      "        [11.9300, 11.9300]])\n"
     ]
    }
   ],
   "source": [
    "inputs = boston['data']\n",
    "X  = torch.FloatTensor([inputs[:,6], inputs[:,2]]).view(506,2)\n",
    "targets  = torch.FloatTensor(boston['target'])\n",
    "x_data = Variable(X)\n",
    "y_data = Variable(targets)\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "los_func = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss 1490.1016845703125\n",
      "epoch 1, loss 328.9723815917969\n",
      "epoch 2, loss 311.944580078125\n",
      "epoch 3, loss 311.502685546875\n",
      "epoch 4, loss 311.30511474609375\n",
      "epoch 5, loss 311.1149597167969\n",
      "epoch 6, loss 310.9286193847656\n",
      "epoch 7, loss 310.74603271484375\n",
      "epoch 8, loss 310.5670166015625\n",
      "epoch 9, loss 310.3915710449219\n",
      "epoch 10, loss 310.21954345703125\n",
      "epoch 11, loss 310.0508728027344\n",
      "epoch 12, loss 309.8854675292969\n",
      "epoch 13, loss 309.7232666015625\n",
      "epoch 14, loss 309.5641784667969\n",
      "epoch 15, loss 309.4081115722656\n",
      "epoch 16, loss 309.2550048828125\n",
      "epoch 17, loss 309.10479736328125\n",
      "epoch 18, loss 308.9573669433594\n",
      "epoch 19, loss 308.81268310546875\n",
      "epoch 20, loss 308.6706848144531\n",
      "epoch 21, loss 308.53131103515625\n",
      "epoch 22, loss 308.3944396972656\n",
      "epoch 23, loss 308.26007080078125\n",
      "epoch 24, loss 308.12811279296875\n",
      "epoch 25, loss 307.99847412109375\n",
      "epoch 26, loss 307.8711853027344\n",
      "epoch 27, loss 307.74609375\n",
      "epoch 28, loss 307.6231994628906\n",
      "epoch 29, loss 307.50244140625\n",
      "epoch 30, loss 307.3837585449219\n",
      "epoch 31, loss 307.26708984375\n",
      "epoch 32, loss 307.15240478515625\n",
      "epoch 33, loss 307.0396423339844\n",
      "epoch 34, loss 306.9287414550781\n",
      "epoch 35, loss 306.8197021484375\n",
      "epoch 36, loss 306.71240234375\n",
      "epoch 37, loss 306.60687255859375\n",
      "epoch 38, loss 306.5030517578125\n",
      "epoch 39, loss 306.4008483886719\n",
      "epoch 40, loss 306.3002624511719\n",
      "epoch 41, loss 306.2012634277344\n",
      "epoch 42, loss 306.1037902832031\n",
      "epoch 43, loss 306.0078430175781\n",
      "epoch 44, loss 305.9132995605469\n",
      "epoch 45, loss 305.8202209472656\n",
      "epoch 46, loss 305.728515625\n",
      "epoch 47, loss 305.6381530761719\n",
      "epoch 48, loss 305.5491027832031\n",
      "epoch 49, loss 305.46136474609375\n",
      "epoch 50, loss 305.3748474121094\n",
      "epoch 51, loss 305.2895812988281\n",
      "epoch 52, loss 305.2054748535156\n",
      "epoch 53, loss 305.12255859375\n",
      "epoch 54, loss 305.040771484375\n",
      "epoch 55, loss 304.9600830078125\n",
      "epoch 56, loss 304.8804626464844\n",
      "epoch 57, loss 304.8019104003906\n",
      "epoch 58, loss 304.724365234375\n",
      "epoch 59, loss 304.6478271484375\n",
      "epoch 60, loss 304.572265625\n",
      "epoch 61, loss 304.4976501464844\n",
      "epoch 62, loss 304.4239807128906\n",
      "epoch 63, loss 304.3511962890625\n",
      "epoch 64, loss 304.279296875\n",
      "epoch 65, loss 304.208251953125\n",
      "epoch 66, loss 304.1380310058594\n",
      "epoch 67, loss 304.0686340332031\n",
      "epoch 68, loss 304.00006103515625\n",
      "epoch 69, loss 303.9322509765625\n",
      "epoch 70, loss 303.86517333984375\n",
      "epoch 71, loss 303.7988586425781\n",
      "epoch 72, loss 303.7332763671875\n",
      "epoch 73, loss 303.6683654785156\n",
      "epoch 74, loss 303.6041564941406\n",
      "epoch 75, loss 303.5406188964844\n",
      "epoch 76, loss 303.47772216796875\n",
      "epoch 77, loss 303.41546630859375\n",
      "epoch 78, loss 303.35382080078125\n",
      "epoch 79, loss 303.29278564453125\n",
      "epoch 80, loss 303.23236083984375\n",
      "epoch 81, loss 303.1724853515625\n",
      "epoch 82, loss 303.1131896972656\n",
      "epoch 83, loss 303.0544128417969\n",
      "epoch 84, loss 302.9961853027344\n",
      "epoch 85, loss 302.9384765625\n",
      "epoch 86, loss 302.88128662109375\n",
      "epoch 87, loss 302.8245849609375\n",
      "epoch 88, loss 302.76837158203125\n",
      "epoch 89, loss 302.7126159667969\n",
      "epoch 90, loss 302.6573181152344\n",
      "epoch 91, loss 302.60247802734375\n",
      "epoch 92, loss 302.5480651855469\n",
      "epoch 93, loss 302.49407958984375\n",
      "epoch 94, loss 302.4405212402344\n",
      "epoch 95, loss 302.3873596191406\n",
      "epoch 96, loss 302.3345947265625\n",
      "epoch 97, loss 302.2822265625\n",
      "epoch 98, loss 302.230224609375\n",
      "epoch 99, loss 302.1785583496094\n",
      "epoch 100, loss 302.1272888183594\n",
      "epoch 101, loss 302.07635498046875\n",
      "epoch 102, loss 302.0257568359375\n",
      "epoch 103, loss 301.97552490234375\n",
      "epoch 104, loss 301.9255676269531\n",
      "epoch 105, loss 301.8759460449219\n",
      "epoch 106, loss 301.82666015625\n",
      "epoch 107, loss 301.77764892578125\n",
      "epoch 108, loss 301.7289123535156\n",
      "epoch 109, loss 301.68048095703125\n",
      "epoch 110, loss 301.63232421875\n",
      "epoch 111, loss 301.58447265625\n",
      "epoch 112, loss 301.5368347167969\n",
      "epoch 113, loss 301.489501953125\n",
      "epoch 114, loss 301.4423828125\n",
      "epoch 115, loss 301.3955383300781\n",
      "epoch 116, loss 301.34893798828125\n",
      "epoch 117, loss 301.30255126953125\n",
      "epoch 118, loss 301.2563781738281\n",
      "epoch 119, loss 301.2104797363281\n",
      "epoch 120, loss 301.1647644042969\n",
      "epoch 121, loss 301.1192626953125\n",
      "epoch 122, loss 301.073974609375\n",
      "epoch 123, loss 301.02886962890625\n",
      "epoch 124, loss 300.9840087890625\n",
      "epoch 125, loss 300.9393005371094\n",
      "epoch 126, loss 300.8948059082031\n",
      "epoch 127, loss 300.8504638671875\n",
      "epoch 128, loss 300.80633544921875\n",
      "epoch 129, loss 300.7623596191406\n",
      "epoch 130, loss 300.71856689453125\n",
      "epoch 131, loss 300.6749267578125\n",
      "epoch 132, loss 300.6314697265625\n",
      "epoch 133, loss 300.5881652832031\n",
      "epoch 134, loss 300.5450134277344\n",
      "epoch 135, loss 300.5019836425781\n",
      "epoch 136, loss 300.4591369628906\n",
      "epoch 137, loss 300.4164123535156\n",
      "epoch 138, loss 300.3738708496094\n",
      "epoch 139, loss 300.3314208984375\n",
      "epoch 140, loss 300.28912353515625\n",
      "epoch 141, loss 300.2469482421875\n",
      "epoch 142, loss 300.20489501953125\n",
      "epoch 143, loss 300.1629943847656\n",
      "epoch 144, loss 300.1211853027344\n",
      "epoch 145, loss 300.0794982910156\n",
      "epoch 146, loss 300.0379638671875\n",
      "epoch 147, loss 299.9964904785156\n",
      "epoch 148, loss 299.9551696777344\n",
      "epoch 149, loss 299.9139404296875\n",
      "epoch 150, loss 299.872802734375\n",
      "epoch 151, loss 299.831787109375\n",
      "epoch 152, loss 299.7908630371094\n",
      "epoch 153, loss 299.75006103515625\n",
      "epoch 154, loss 299.7093200683594\n",
      "epoch 155, loss 299.668701171875\n",
      "epoch 156, loss 299.6281433105469\n",
      "epoch 157, loss 299.58770751953125\n",
      "epoch 158, loss 299.5473327636719\n",
      "epoch 159, loss 299.5070495605469\n",
      "epoch 160, loss 299.46685791015625\n",
      "epoch 161, loss 299.4267578125\n",
      "epoch 162, loss 299.3866882324219\n",
      "epoch 163, loss 299.34674072265625\n",
      "epoch 164, loss 299.3068542480469\n",
      "epoch 165, loss 299.2670593261719\n",
      "epoch 166, loss 299.2273254394531\n",
      "epoch 167, loss 299.1876525878906\n",
      "epoch 168, loss 299.1480712890625\n",
      "epoch 169, loss 299.1085205078125\n",
      "epoch 170, loss 299.0690612792969\n",
      "epoch 171, loss 299.0296630859375\n",
      "epoch 172, loss 298.9903259277344\n",
      "epoch 173, loss 298.9510498046875\n",
      "epoch 174, loss 298.911865234375\n",
      "epoch 175, loss 298.8727111816406\n",
      "epoch 176, loss 298.8335876464844\n",
      "epoch 177, loss 298.7945556640625\n",
      "epoch 178, loss 298.7555847167969\n",
      "epoch 179, loss 298.7166442871094\n",
      "epoch 180, loss 298.6777648925781\n",
      "epoch 181, loss 298.6389465332031\n",
      "epoch 182, loss 298.60015869140625\n",
      "epoch 183, loss 298.5614318847656\n",
      "epoch 184, loss 298.52276611328125\n",
      "epoch 185, loss 298.484130859375\n",
      "epoch 186, loss 298.445556640625\n",
      "epoch 187, loss 298.4070129394531\n",
      "epoch 188, loss 298.3684997558594\n",
      "epoch 189, loss 298.3300476074219\n",
      "epoch 190, loss 298.2916564941406\n",
      "epoch 191, loss 298.2532653808594\n",
      "epoch 192, loss 298.2149658203125\n",
      "epoch 193, loss 298.1766662597656\n",
      "epoch 194, loss 298.138427734375\n",
      "epoch 195, loss 298.1002197265625\n",
      "epoch 196, loss 298.0620422363281\n",
      "epoch 197, loss 298.0238952636719\n",
      "epoch 198, loss 297.98577880859375\n",
      "epoch 199, loss 297.9477233886719\n",
      "epoch 200, loss 297.9096984863281\n",
      "epoch 201, loss 297.8717041015625\n",
      "epoch 202, loss 297.8337097167969\n",
      "epoch 203, loss 297.7958068847656\n",
      "epoch 204, loss 297.7579040527344\n",
      "epoch 205, loss 297.7200012207031\n",
      "epoch 206, loss 297.68218994140625\n",
      "epoch 207, loss 297.6443786621094\n",
      "epoch 208, loss 297.6065979003906\n",
      "epoch 209, loss 297.5688171386719\n",
      "epoch 210, loss 297.5310974121094\n",
      "epoch 211, loss 297.493408203125\n",
      "epoch 212, loss 297.45574951171875\n",
      "epoch 213, loss 297.4180908203125\n",
      "epoch 214, loss 297.3804626464844\n",
      "epoch 215, loss 297.3428649902344\n",
      "epoch 216, loss 297.3052978515625\n",
      "epoch 217, loss 297.26776123046875\n",
      "epoch 218, loss 297.2302551269531\n",
      "epoch 219, loss 297.1927490234375\n",
      "epoch 220, loss 297.1552734375\n",
      "epoch 221, loss 297.1178283691406\n",
      "epoch 222, loss 297.08038330078125\n",
      "epoch 223, loss 297.0429992675781\n",
      "epoch 224, loss 297.005615234375\n",
      "epoch 225, loss 296.9682312011719\n",
      "epoch 226, loss 296.930908203125\n",
      "epoch 227, loss 296.8935852050781\n",
      "epoch 228, loss 296.8562927246094\n",
      "epoch 229, loss 296.8190002441406\n",
      "epoch 230, loss 296.78173828125\n",
      "epoch 231, loss 296.7445068359375\n",
      "epoch 232, loss 296.707275390625\n",
      "epoch 233, loss 296.6700744628906\n",
      "epoch 234, loss 296.63287353515625\n",
      "epoch 235, loss 296.595703125\n",
      "epoch 236, loss 296.55853271484375\n",
      "epoch 237, loss 296.52142333984375\n",
      "epoch 238, loss 296.4842834472656\n",
      "epoch 239, loss 296.44720458984375\n",
      "epoch 240, loss 296.41009521484375\n",
      "epoch 241, loss 296.373046875\n",
      "epoch 242, loss 296.33599853515625\n",
      "epoch 243, loss 296.2989501953125\n",
      "epoch 244, loss 296.2619323730469\n",
      "epoch 245, loss 296.22491455078125\n",
      "epoch 246, loss 296.18792724609375\n",
      "epoch 247, loss 296.15093994140625\n",
      "epoch 248, loss 296.1139831542969\n",
      "epoch 249, loss 296.0770263671875\n",
      "epoch 250, loss 296.04010009765625\n",
      "epoch 251, loss 296.003173828125\n",
      "epoch 252, loss 295.9662780761719\n",
      "epoch 253, loss 295.92938232421875\n",
      "epoch 254, loss 295.89251708984375\n",
      "epoch 255, loss 295.85565185546875\n",
      "epoch 256, loss 295.81878662109375\n",
      "epoch 257, loss 295.7819519042969\n",
      "epoch 258, loss 295.7451171875\n",
      "epoch 259, loss 295.70831298828125\n",
      "epoch 260, loss 295.6715087890625\n",
      "epoch 261, loss 295.63470458984375\n",
      "epoch 262, loss 295.5979309082031\n",
      "epoch 263, loss 295.5611572265625\n",
      "epoch 264, loss 295.5244140625\n",
      "epoch 265, loss 295.4876708984375\n",
      "epoch 266, loss 295.450927734375\n",
      "epoch 267, loss 295.4142150878906\n",
      "epoch 268, loss 295.37750244140625\n",
      "epoch 269, loss 295.3408203125\n",
      "epoch 270, loss 295.30413818359375\n",
      "epoch 271, loss 295.2674560546875\n",
      "epoch 272, loss 295.2308044433594\n",
      "epoch 273, loss 295.19415283203125\n",
      "epoch 274, loss 295.1575012207031\n",
      "epoch 275, loss 295.1208801269531\n",
      "epoch 276, loss 295.0842590332031\n",
      "epoch 277, loss 295.0476379394531\n",
      "epoch 278, loss 295.01104736328125\n",
      "epoch 279, loss 294.9744567871094\n",
      "epoch 280, loss 294.9378662109375\n",
      "epoch 281, loss 294.90130615234375\n",
      "epoch 282, loss 294.86474609375\n",
      "epoch 283, loss 294.82818603515625\n",
      "epoch 284, loss 294.7916259765625\n",
      "epoch 285, loss 294.7550964355469\n",
      "epoch 286, loss 294.7185974121094\n",
      "epoch 287, loss 294.68206787109375\n",
      "epoch 288, loss 294.64556884765625\n",
      "epoch 289, loss 294.60906982421875\n",
      "epoch 290, loss 294.57257080078125\n",
      "epoch 291, loss 294.5361022949219\n",
      "epoch 292, loss 294.4996337890625\n",
      "epoch 293, loss 294.4631652832031\n",
      "epoch 294, loss 294.4267272949219\n",
      "epoch 295, loss 294.3902893066406\n",
      "epoch 296, loss 294.3538513183594\n",
      "epoch 297, loss 294.31744384765625\n",
      "epoch 298, loss 294.281005859375\n",
      "epoch 299, loss 294.2445983886719\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300): \n",
    "    pred_y = model(x_data) \n",
    "    loss = los_func(pred_y, y_data) \n",
    "    optimizer.zero_grad() \n",
    "    loss.backward() \n",
    "    optimizer.step() \n",
    "    print('epoch {}, loss {}'.format(epoch, loss.data)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
